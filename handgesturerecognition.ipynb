{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikyl\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os,sys\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "#from IPython.display import Image\n",
    "from keras import applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we try to resize all the images to 100x100 pixel size, we aren't reducing the size since that might leads to information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "handgesturedata/A/a01.bmp\n",
      "handgesturedata/A/a02.bmp\n",
      "handgesturedata/A/a03.bmp\n",
      "handgesturedata/A/a04.bmp\n",
      "handgesturedata/A/a05.bmp\n",
      "handgesturedata/A/a06.bmp\n",
      "handgesturedata/A/a07.bmp\n",
      "handgesturedata/A/a08.bmp\n",
      "handgesturedata/A/a09.bmp\n",
      "handgesturedata/A/a10.bmp\n",
      "handgesturedata/A/a11.bmp\n",
      "handgesturedata/A/a12.bmp\n",
      "handgesturedata/A/a13.bmp\n",
      "handgesturedata/A/a14.bmp\n",
      "handgesturedata/A/a15.bmp\n",
      "handgesturedata/A/a16.bmp\n",
      "handgesturedata/A/a17.bmp\n",
      "handgesturedata/A/a18.bmp\n",
      "B\n",
      "handgesturedata/B/b01.bmp\n",
      "handgesturedata/B/b02.bmp\n",
      "handgesturedata/B/b03.bmp\n",
      "handgesturedata/B/b04.bmp\n",
      "handgesturedata/B/b05.bmp\n",
      "handgesturedata/B/b06.bmp\n",
      "handgesturedata/B/b07.bmp\n",
      "handgesturedata/B/b08.bmp\n",
      "handgesturedata/B/b09.bmp\n",
      "handgesturedata/B/b10.bmp\n",
      "handgesturedata/B/b11.bmp\n",
      "handgesturedata/B/b12.bmp\n",
      "handgesturedata/B/b13.bmp\n",
      "handgesturedata/B/b14.bmp\n",
      "handgesturedata/B/b15.bmp\n",
      "handgesturedata/B/b16.bmp\n",
      "handgesturedata/B/b17.bmp\n",
      "handgesturedata/B/b18.bmp\n",
      "handgesturedata/B/b19.bmp\n",
      "C\n",
      "handgesturedata/C/c01.bmp\n",
      "handgesturedata/C/c02.bmp\n",
      "handgesturedata/C/c03.bmp\n",
      "handgesturedata/C/c04.bmp\n",
      "handgesturedata/C/c05.bmp\n",
      "handgesturedata/C/c06.bmp\n",
      "handgesturedata/C/c07.bmp\n",
      "handgesturedata/C/c08.bmp\n",
      "handgesturedata/C/c09.bmp\n",
      "handgesturedata/C/c10.bmp\n",
      "handgesturedata/C/c11.bmp\n",
      "handgesturedata/C/c12.bmp\n",
      "handgesturedata/C/c13.bmp\n",
      "handgesturedata/C/c14.bmp\n",
      "D\n",
      "handgesturedata/D/d01.bmp\n",
      "handgesturedata/D/d02.bmp\n",
      "handgesturedata/D/d03.bmp\n",
      "handgesturedata/D/d04.bmp\n",
      "handgesturedata/D/d05.bmp\n",
      "handgesturedata/D/d06.bmp\n",
      "handgesturedata/D/d07.bmp\n",
      "handgesturedata/D/d08.bmp\n",
      "handgesturedata/D/d09.bmp\n",
      "handgesturedata/D/d10.bmp\n",
      "handgesturedata/D/d11.bmp\n",
      "handgesturedata/D/d12.bmp\n",
      "handgesturedata/D/d13.bmp\n",
      "handgesturedata/D/d14.bmp\n",
      "handgesturedata/D/d15.bmp\n",
      "handgesturedata/D/d16.bmp\n",
      "handgesturedata/D/d17.bmp\n",
      "NA\n",
      "handgesturedata/NA/1.bmp\n",
      "handgesturedata/NA/10.bmp\n",
      "handgesturedata/NA/11.bmp\n",
      "handgesturedata/NA/12.bmp\n",
      "handgesturedata/NA/13.bmp\n",
      "handgesturedata/NA/14.bmp\n",
      "handgesturedata/NA/15.bmp\n",
      "handgesturedata/NA/16.bmp\n",
      "handgesturedata/NA/17.bmp\n",
      "handgesturedata/NA/2.bmp\n",
      "handgesturedata/NA/3.bmp\n",
      "handgesturedata/NA/4.bmp\n",
      "handgesturedata/NA/5.bmp\n",
      "handgesturedata/NA/6.bmp\n",
      "handgesturedata/NA/7.bmp\n",
      "handgesturedata/NA/8.bmp\n",
      "handgesturedata/NA/9.bmp\n"
     ]
    }
   ],
   "source": [
    "path = (\"handgesturedata/\")\n",
    "if not os.path.exists(\"preview\"):\n",
    "    os.makedirs(\"preview\")\n",
    "img_width, img_height = 100, 100\n",
    "batchsize=32\n",
    "for subfolder in os.listdir(path):\n",
    "    print(subfolder)\n",
    "    for item in os.listdir(path+subfolder):\n",
    "        #print(item)\n",
    "        if item.endswith(\".bmp\"):\n",
    "            im = Image.open(path+subfolder+\"/\"+item)\n",
    "            print(path+subfolder+\"/\"+item)\n",
    "            f, e = os.path.split(path+subfolder+\"/\"+item)\n",
    "            imResize = im.resize((100,100), Image.ANTIALIAS)\n",
    "            im.close()\n",
    "            imResize.save(path+subfolder+\"/\"+item,\"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we divide the dataset into training and validation set for training a convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "['a01.bmp', 'a02.bmp', 'a03.bmp', 'a04.bmp', 'a05.bmp', 'a06.bmp', 'a07.bmp', 'a08.bmp', 'a09.bmp', 'a10.bmp', 'a11.bmp', 'a12.bmp']\n",
      "['a13.bmp', 'a14.bmp', 'a15.bmp', 'a16.bmp', 'a17.bmp', 'a18.bmp']\n",
      "B\n",
      "['b01.bmp', 'b02.bmp', 'b03.bmp', 'b04.bmp', 'b05.bmp', 'b06.bmp', 'b07.bmp', 'b08.bmp', 'b09.bmp', 'b10.bmp', 'b11.bmp', 'b12.bmp']\n",
      "['b13.bmp', 'b14.bmp', 'b15.bmp', 'b16.bmp', 'b17.bmp', 'b18.bmp', 'b19.bmp']\n",
      "C\n",
      "['c01.bmp', 'c02.bmp', 'c03.bmp', 'c04.bmp', 'c05.bmp', 'c06.bmp', 'c07.bmp', 'c08.bmp', 'c09.bmp']\n",
      "['c10.bmp', 'c11.bmp', 'c12.bmp', 'c13.bmp', 'c14.bmp']\n",
      "D\n",
      "['d01.bmp', 'd02.bmp', 'd03.bmp', 'd04.bmp', 'd05.bmp', 'd06.bmp', 'd07.bmp', 'd08.bmp', 'd09.bmp', 'd10.bmp', 'd11.bmp']\n",
      "['d12.bmp', 'd13.bmp', 'd14.bmp', 'd15.bmp', 'd16.bmp', 'd17.bmp']\n",
      "NA\n",
      "['1.bmp', '10.bmp', '11.bmp', '12.bmp', '13.bmp', '14.bmp', '15.bmp', '16.bmp', '17.bmp', '2.bmp', '3.bmp']\n",
      "['4.bmp', '5.bmp', '6.bmp', '7.bmp', '8.bmp', '9.bmp']\n"
     ]
    }
   ],
   "source": [
    "for subfolder in os.listdir(path):\n",
    "    print(subfolder)\n",
    "    trainingset = os.listdir(path+subfolder)[:2*len(os.listdir(path+subfolder))//3]\n",
    "    validationset = os.listdir(path+subfolder)[2*len(os.listdir(path+subfolder))//3:]\n",
    "    print(trainingset)\n",
    "    print(validationset)\n",
    "    if not os.path.exists(\"data/train/\"+subfolder):\n",
    "        os.makedirs(\"data/train/\"+subfolder)\n",
    "        os.makedirs(\"data/validation/\"+subfolder)\n",
    "        \n",
    "    for item in trainingset:\n",
    "        shutil.copy2(path+subfolder+'/'+item,\"data/train/\"+subfolder)\n",
    "        \n",
    "    for item in validationset:\n",
    "        shutil.copy2(path+subfolder+'/'+item,\"data/validation/\"+subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55 images belonging to 5 classes.\n",
      "(1, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "train_generator = ImageDataGenerator(width_shift_range = 0.1,height_shift_range = 0.1, horizontal_flip=True,shear_range=0.1)\n",
    "\n",
    "train = train_generator.flow_from_directory(\"data/train\",target_size=(img_height,img_width),batch_size=32,class_mode='categorical')\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(figsize=(10,10))\n",
    "for image in enumerate(train_generator):\n",
    "    plt.subplot(6,6)\n",
    "    plt.imshow(image)\n",
    "\"\"\"\n",
    "\n",
    "img=load_img(\"data/train/A/a01.bmp\")\n",
    "x=img_to_array(img)\n",
    "x=x.reshape((1,)+x.shape)\n",
    "print(x.shape)\n",
    "i=0\n",
    "for batch in train_generator.flow(x,batch_size=1, save_to_dir='preview', save_prefix='preview', save_format='bmp'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = ImageDataGenerator()\n",
    "validation = val_generator.flow_from_directory('data/validation',target_size=(img_height,img_width),batch_size=32,class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 100, 100, 3)       400       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 98, 98, 32)        392       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 47, 47, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 47, 47, 64)        188       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 21, 21, 128)       84        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 3,440,493\n",
      "Trainable params: 3,438,937\n",
      "Non-trainable params: 1,556\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(axis=1, input_shape=(img_width,img_height,3)),\n",
    "    Convolution2D(32, (3,3), activation='relu'),\n",
    "    BatchNormalization(axis=1),\n",
    "    MaxPooling2D(),\n",
    "    Convolution2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(axis=1),\n",
    "    MaxPooling2D(),\n",
    "    Convolution2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(axis=1),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(5, activation='sigmoid')\n",
    "])\n",
    "model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2.2007 - acc: 0.1250 - val_loss: 1.7082 - val_acc: 0.2667\n",
      "Epoch 2/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.4561 - acc: 0.2174 - val_loss: 1.5009 - val_acc: 0.3667\n",
      "Epoch 3/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.9774 - acc: 0.2812 - val_loss: 1.3182 - val_acc: 0.4667\n",
      "Epoch 4/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.3712 - acc: 0.4783 - val_loss: 1.2367 - val_acc: 0.5000\n",
      "Epoch 5/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2690 - acc: 0.4783 - val_loss: 1.1845 - val_acc: 0.6667\n",
      "Epoch 6/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.3878 - acc: 0.5000 - val_loss: 1.1283 - val_acc: 0.7333\n",
      "Epoch 7/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2651 - acc: 0.4688 - val_loss: 1.0840 - val_acc: 0.7333\n",
      "Epoch 8/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2441 - acc: 0.5217 - val_loss: 1.0386 - val_acc: 0.8333\n",
      "Epoch 9/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2997 - acc: 0.4348 - val_loss: 1.0056 - val_acc: 0.8333\n",
      "Epoch 10/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2250 - acc: 0.5000 - val_loss: 0.9799 - val_acc: 0.8667\n",
      "Epoch 11/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1214 - acc: 0.5938 - val_loss: 0.9628 - val_acc: 0.9000\n",
      "Epoch 12/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1826 - acc: 0.5652 - val_loss: 0.9516 - val_acc: 0.9000\n",
      "Epoch 13/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2289 - acc: 0.6562 - val_loss: 0.9303 - val_acc: 0.9000\n",
      "Epoch 14/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9964 - acc: 0.6522 - val_loss: 0.9072 - val_acc: 0.9000\n",
      "Epoch 15/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0520 - acc: 0.6957 - val_loss: 0.8944 - val_acc: 0.9000\n",
      "Epoch 16/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0966 - acc: 0.5938 - val_loss: 0.8828 - val_acc: 0.9000\n",
      "Epoch 17/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1016 - acc: 0.7188 - val_loss: 0.8712 - val_acc: 0.9000\n",
      "Epoch 18/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9638 - acc: 0.8261 - val_loss: 0.8606 - val_acc: 0.9000\n",
      "Epoch 19/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9695 - acc: 0.8750 - val_loss: 0.8513 - val_acc: 0.9333\n",
      "Epoch 20/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9749 - acc: 0.7826 - val_loss: 0.8464 - val_acc: 0.9333\n",
      "Epoch 21/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0413 - acc: 0.8125 - val_loss: 0.8413 - val_acc: 0.9333\n",
      "Epoch 22/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0208 - acc: 0.7391 - val_loss: 0.8315 - val_acc: 0.9667\n",
      "Epoch 23/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0168 - acc: 0.7391 - val_loss: 0.8238 - val_acc: 0.9667\n",
      "Epoch 24/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9542 - acc: 0.8125 - val_loss: 0.8183 - val_acc: 0.9667\n",
      "Epoch 25/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9233 - acc: 0.9565 - val_loss: 0.8136 - val_acc: 0.9667\n",
      "Epoch 26/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9618 - acc: 0.8125 - val_loss: 0.8073 - val_acc: 0.9667\n",
      "Epoch 27/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9568 - acc: 0.7391 - val_loss: 0.8028 - val_acc: 0.9667\n",
      "Epoch 28/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9359 - acc: 0.8438 - val_loss: 0.7983 - val_acc: 0.9667\n",
      "Epoch 29/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8704 - acc: 0.7826 - val_loss: 0.7940 - val_acc: 0.9667\n",
      "Epoch 30/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9714 - acc: 0.7500 - val_loss: 0.7872 - val_acc: 0.9667\n",
      "Epoch 31/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8379 - acc: 0.8750 - val_loss: 0.7834 - val_acc: 0.9667\n",
      "Epoch 32/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8791 - acc: 0.8696 - val_loss: 0.7804 - val_acc: 0.9667\n",
      "Epoch 33/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8582 - acc: 0.7500 - val_loss: 0.7741 - val_acc: 0.9667\n",
      "Epoch 34/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8201 - acc: 0.8261 - val_loss: 0.7716 - val_acc: 0.9667\n",
      "Epoch 35/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8927 - acc: 0.8125 - val_loss: 0.7686 - val_acc: 0.9667\n",
      "Epoch 36/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9043 - acc: 0.9130 - val_loss: 0.7649 - val_acc: 0.9333\n",
      "Epoch 37/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0029 - acc: 0.8261 - val_loss: 0.7571 - val_acc: 0.9333\n",
      "Epoch 38/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8396 - acc: 0.9062 - val_loss: 0.7558 - val_acc: 0.9333\n",
      "Epoch 39/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8636 - acc: 0.9130 - val_loss: 0.7488 - val_acc: 0.9333\n",
      "Epoch 40/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8802 - acc: 0.8750 - val_loss: 0.7440 - val_acc: 0.9333\n",
      "Epoch 41/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9099 - acc: 0.8261 - val_loss: 0.7391 - val_acc: 0.9333\n",
      "Epoch 42/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7893 - acc: 0.9375 - val_loss: 0.7346 - val_acc: 0.9333\n",
      "Epoch 43/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8876 - acc: 0.7826 - val_loss: 0.7280 - val_acc: 0.9333\n",
      "Epoch 44/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8382 - acc: 0.8438 - val_loss: 0.7249 - val_acc: 0.9333\n",
      "Epoch 45/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8533 - acc: 0.8696 - val_loss: 0.7201 - val_acc: 0.9333\n",
      "Epoch 46/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8397 - acc: 0.8438 - val_loss: 0.7174 - val_acc: 0.9333\n",
      "Epoch 47/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8538 - acc: 0.8261 - val_loss: 0.7120 - val_acc: 0.9333\n",
      "Epoch 48/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8638 - acc: 0.9688 - val_loss: 0.7058 - val_acc: 0.9333\n",
      "Epoch 49/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7814 - acc: 0.9130 - val_loss: 0.7010 - val_acc: 0.9333\n",
      "Epoch 50/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9120 - acc: 0.8438 - val_loss: 0.6941 - val_acc: 0.9333\n",
      "Epoch 51/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8154 - acc: 0.9688 - val_loss: 0.6873 - val_acc: 0.9333\n",
      "Epoch 52/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8051 - acc: 0.8696 - val_loss: 0.6825 - val_acc: 0.9333\n",
      "Epoch 53/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8515 - acc: 0.9375 - val_loss: 0.6774 - val_acc: 0.9333\n",
      "Epoch 54/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8295 - acc: 0.9565 - val_loss: 0.6730 - val_acc: 0.9333\n",
      "Epoch 55/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7707 - acc: 0.9130 - val_loss: 0.6692 - val_acc: 0.9333\n",
      "Epoch 56/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8022 - acc: 0.9375 - val_loss: 0.6649 - val_acc: 0.9333\n",
      "Epoch 57/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8015 - acc: 0.9688 - val_loss: 0.6614 - val_acc: 0.9333\n",
      "Epoch 58/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7717 - acc: 0.9130 - val_loss: 0.6624 - val_acc: 0.9333\n",
      "Epoch 59/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7927 - acc: 0.8696 - val_loss: 0.6591 - val_acc: 0.9333\n",
      "Epoch 60/60\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7662 - acc: 0.9062 - val_loss: 0.6560 - val_acc: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b50faca438>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train,steps_per_epoch=train.samples//32,epochs=60,validation_data=validation,validation_steps=validation.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('augmented_result.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85 images belonging to 5 classes.\n",
      "Shape of data after flattening the image:  (1020, 30000)\n",
      "Shape of the target value:  [1 2 3 ... 4 4 4]\n",
      "Shape of data after feature selection:  (1020, 13434)\n",
      "Shape of data after applying PCA:  (1020, 1000)\n"
     ]
    }
   ],
   "source": [
    "totalepochs=100\n",
    "epoch=0\n",
    "train = train_generator.flow_from_directory(\"Handgesturedata/\",target_size=(img_height,img_width),batch_size=500,class_mode='sparse')\n",
    "for i,j in enumerate(train):\n",
    "    X = j[0]\n",
    "    y= j[1]\n",
    "    break;\n",
    "counter=0\n",
    "for i,j in enumerate(train):\n",
    "    if counter > 10:\n",
    "        break\n",
    "    X = np.append(X, j[0],axis=0)\n",
    "    y = np.append(y, j[1],axis=0)\n",
    "    counter+=1\n",
    "\n",
    "#use Sklearn algorithms here\n",
    "X = X.reshape((len(X),-1))\n",
    "print(\"Shape of data after flattening the image: \", X.shape)\n",
    "print(\"Shape of the target value: \", y)\n",
    "\n",
    "#Polynomial features expansion to degree 2\n",
    "#poly = PolynomialFeatures(degree=2,interaction_only=True)\n",
    "#X = poly.fit_transform(X)\n",
    "\n",
    "#radial basis function kernel over the transformed data\n",
    "#rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "#X = rbf_feature.fit_transform(X)\n",
    "\n",
    "\n",
    "#feature selection using Lasso\n",
    "lsvc = svm.LinearSVC(C=0.01).fit(X,y)\n",
    "skmodel = SelectFromModel(lsvc, prefit=True)\n",
    "#ard = ARDRegression()\n",
    "#skmodel = SelectFromModel(ard, prefit=True)\n",
    "X = skmodel.transform(X)\n",
    "\n",
    "print(\"Shape of data after feature selection: \", X.shape)\n",
    "#we intiate the classifier objects here\n",
    "#decreasing the number of principal components to be lesser than the number of training examples.\n",
    "pca = PCA(n_components=1000)\n",
    "X=pca.fit_transform(X)\n",
    "\n",
    "print(\"Shape of data after applying PCA: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=None, shuffle=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikyl\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.05882352941177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikyl\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.23529411764706\n",
      "97.6470588235294\n",
      "Accuracy:  97.6470588235294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikyl\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "kf = KFold(n_splits=3)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)\n",
    "finalaccuracy=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    xgmodel = XGBClassifier()\n",
    "    xgmodel.fit(X_train, y_train)\n",
    "    y_pred = xgmodel.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(accuracy*100.0)\n",
    "    finalaccuracy.append(accuracy*100.0)\n",
    "print(\"Accuracy: \", sum(finalaccuracy)/float(len(finalaccuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.235294117647058\n",
      "70.0\n",
      "96.76470588235294\n",
      "Accuracy:  63.333333333333336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "finalaccuracy=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y0_train, y_test = y[train_index], y[test_index]\n",
    "    svmadamodel = AdaBoostClassifier(base_estimator=svm.SVC(probability=True, kernel=\"poly\", degree=5), n_estimators=50)\n",
    "    svmadamodel.fit(X_train, y_train)\n",
    "    y_pred = svmadamodel.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(accuracy*100.0)\n",
    "    finalaccuracy.append(accuracy*100.0)\n",
    "print(\"Accuracy: \", sum(finalaccuracy)/float(len(finalaccuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.23529411764706\n",
      "90.58823529411765\n",
      "90.29411764705883\n",
      "Accuracy:  91.37254901960785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "finalaccuracy=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    rfcmodel =  RandomForestClassifier(n_estimators=100)\n",
    "    rfcmodel.fit(X_train, y_train)\n",
    "    y_pred = rfcmodel.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(accuracy*100.0)\n",
    "    finalaccuracy.append(accuracy*100.0)\n",
    "print(\"Accuracy: \", sum(finalaccuracy)/float(len(finalaccuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load the test pictures and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "test/A/a01.bmp\n",
      "test/A/a02.bmp\n",
      "B\n",
      "test/B/b01.bmp\n",
      "test/B/b02.bmp\n",
      "C\n",
      "test/C/c01.bmp\n",
      "test/C/c02.bmp\n",
      "D\n",
      "test/D/d01.bmp\n",
      "test/D/d02.bmp\n",
      "NA\n",
      "test/NA/1.bmp\n",
      "test/NA/2.bmp\n"
     ]
    }
   ],
   "source": [
    "path = (\"test/\")\n",
    "img_width, img_height = 100, 100\n",
    "batchsize=32\n",
    "for subfolder in os.listdir(path):\n",
    "    print(subfolder)\n",
    "    for item in os.listdir(path+subfolder):\n",
    "        #print(item)\n",
    "        if item.endswith(\".bmp\"):\n",
    "            im = Image.open(path+subfolder+\"/\"+item)\n",
    "            print(path+subfolder+\"/\"+item)\n",
    "            f, e = os.path.split(path+subfolder+\"/\"+item)\n",
    "            imResize = im.resize((100,100), Image.ANTIALIAS)\n",
    "            im.close()\n",
    "            imResize.save(path+subfolder+\"/\"+item,\"JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 5 classes.\n",
      "(10, 100, 100, 3)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "epoch=0\n",
    "test_generator = ImageDataGenerator()\n",
    "test = test_generator.flow_from_directory(\"test/\",target_size=(img_height,img_width),batch_size=10,class_mode='categorical')\n",
    "for i,j in enumerate(test):\n",
    "    X_test = j[0]\n",
    "    y_test = j[1]\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    break;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the test data on the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.60420686, 1.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.test_on_batch(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "epoch=0\n",
    "test_generator = ImageDataGenerator()\n",
    "test = test_generator.flow_from_directory(\"test/\",target_size=(img_height,img_width),batch_size=10,class_mode='sparse')\n",
    "for i,j in enumerate(test):\n",
    "    X_test = j[0]\n",
    "    y_test = j[1]\n",
    "    break;\n",
    "\n",
    "#use Sklearn algorithms here\n",
    "X_test2 = X_test.reshape((len(X_test),-1))\n",
    "\n",
    "#ard = ARDRegression()\n",
    "#skmodel = SelectFromModel(ard, prefit=True)\n",
    "X_test2 = skmodel.transform(X_test2)\n",
    "#we intiate the classifier objects here\n",
    "#decreasing the number of principal components to be lesser than the number of training examples.\n",
    "#pca = PCA(n_components=1000)\n",
    "X_test2=pca.transform(X_test2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test the data on non Neural Network based algorithms which were trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "90.0\n",
      "70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikyl\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#We test XGboost classifier on the test set\n",
    "y_pred = xgmodel.predict(X_test2)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy*100.0)\n",
    "\n",
    "#We test SVM with adaboost on the test set\n",
    "y_pred = svmadamodel.predict(X_test2)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy*100.0)\n",
    "\n",
    "#We train random forest classifier on the test set\n",
    "y_pred = rfcmodel.predict(X_test2)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
